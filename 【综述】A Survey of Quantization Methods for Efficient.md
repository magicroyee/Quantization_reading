# 摘要

随着抽象的数学计算被应用于数字计算机的计算，有效的表示、处理和交流这些数值的问题凸显了出来。量化的问题和数值表示的问题有着很强的关联性，一个连续的实数的集合应该被以什么方式分配到一个固定的离散的数字集合上，以最小化所需的比特数并且也保持随之产生的计算的准确性。这个常见量化问题当内存或者计算资源严重受限时尤为重要，并且这个问题近些年已经走到了前台，由于神经网络模型在计算机视觉、自然语言处理和相关领域上的突出的表现。从浮点表示转换到用4位或更少的位表示的低精度固定整数值，有可能将内存占用和延迟降低16倍。实际上，在这些应用实践中，经常实现降低4倍到8倍。因此，在和神经网络相关的计算的高效实现中，量化最近成为了一个重要的并且非常活跃的子领域，并不令人惊讶。本文中，我们调查了神经网络计算领域中的数值量化问题的处理方式，包括近些年方法的优点和缺点。通过这篇调研和它的组织，我们希望形成一个有用的印象关于现在的神经网络量化研究，并且给出一个智能的体系去简化对该领域的未来研究的评估。

# 介绍

需要重新考虑设计、训练、部署。

## 设计高效的神经网络模型

- 优化神经网络模型的架构
	- 微架构
	- 宏架构
- 自动机器学习(AutoML)
- 网络架构搜索(NAS)

## 协同设计NN架构和硬件

专用cache

## 剪枝

- 非结构剪枝，删去低特征神经元
	- 稀疏矩阵计算
- 结构剪枝，改变每层的结构

## 知识蒸馏

训练一个教师网络，再用它训练一个更紧凑的网络模型。教师网络产生“软”可能性，用其训练子类模型。

知识蒸馏和其他方法结合取得了巨大成功。

## 量化

量化在NN模型的训练和推理过程中取得了巨大的以及持续性的成功。已经被证明了很难达到半精度之下。

这篇文章的焦点是推理量化。

## 量化和神经科学

和神经网络有些许相关性的工作是神经科学，它踩车人类大脑以一种离散或者量化的方式存储信息，而不是连续的方式。对于这个观点，一个流行的基础理论是以连续形式存储的信息，必然会被噪声干扰。也有其他的观点，包括离散形式的更高泛化能力以及受限资源条件下的更高的效能。

这篇文章的目的是介绍现在的量化中的方式和概念，并且讨论现在研究中的挑战和机遇。

# 量化简史

## 量化的历史

1998年，Gray和Neuhoff写了一篇非常好的量化历史综述。这篇文章是非常好的，并且值得全部读完。

量化是一种将大集合中的输入数值映射到小集合中的输出数值的方法，有着很长的历史。取整和截断是典型的例子。量化和微积分的基础相关，并且相关的方法可以追溯到1800年代，例如早期对于最小二乘和大尺度数据分析的相关技术的工作。一个早期量化的工作可以追溯到1867年，当时离散被用于近似整数计算。随后，在1897年，当时Shappard调研了整数结果上的取整误差的影响。更近一些，量化在数字信号处理领域十分重要，因为处理数字信号的过程经常包括四舍五入，同时也有一个数值分析以及数值算法的应用，这些实数运算在有限精度的算法上进行。

到了1948年，大概在数字计算机发明前后，Shannon写了通信中数学理论的奠基性论文，量化的力量以及它在编码理论的应用正式被提出了。特别地，Shannon在他的无损编码理论中提到了使用相同的比特数是浪费的，当关注的事件有非均匀概率时。他提出一个更可选的方式是根据一个事物的概率去改变比特数，这种方法被称为变长编码。霍夫曼编码就是由这个驱动的。在后续的工作中，香农引入了失真率函数（这提供了一个编码后信号失真的下界）以及向量量化的概念。这个概念被拓展了并且对于实际的通信应用中变实用了。其他那个时期在信号处理领域历史性的对于量化的研究还有【188】，它引入了脉冲编码调制概念，以及高分辨率量化【14】。有兴趣的读者可以去看看【76】。

在对于问题的数学近似算法中，量化以一种不同的方式出现。在数值分析中，一个重要的概念是一个问题是合理的，前提是：一个解存在；解释唯一的；并且解在合理的空间中连续地依赖于输入。这样的问题有时被称作状态良好的问题。可以证明，即使在处理良好条件问题时，某种算法在理想条件下精确地解决了这个问题，但是在有舍入和截断误差所引入的噪声条件下表现十分糟糕。这些舍入误差与使用有限的比特数表示实数有关，例如IEEE浮点标准指明的量化，以及截断误差是因为迭代算法的迭代次数有限。后者在“精确计算”中也很重要，因为大多数连续的数学问题甚至在原理上不能被一个有限的元素运算序列求解，但是前者和量化有关。这些问题导致了一个算法的算法稳定性的概念。让我们将一个数值算法看做一个把输入$x$映射到“真”解$y$的函数$f$。但是由于舍入和截断误差，算法的输出实际是$y^*$。这样，算法的前向误差就是$\Delta y=y^*-y$；反向误差是使得$f(x+\Delta x)=y^*$的最小的$\Delta x$。这样，前向误差告诉我们精确解和算法的输出之间的误差。反向误差告诉我们求到了哪个输入数据的解。一个算法的前向误差和后向误差是通过问题的条件数联系在一起的。读者可以去【237】获得更详细的讨论。

## 神经网络中的量化

无疑有很多文章已经写过这个话题，可能有人会问：近期关于神经网络量化的工作和以往的工作有什么不同呢？可以肯定的是，大量的近期提出的“新颖算法”和文献中的以往工作有很强的相关性。然而，神经网络给量化的问题带来了独特的挑战和机遇。首先，推理和训练都是计算密集型。所以有效的数值表示是尤其重要的。其次，大部分现在的神经网络是严重过参数化的，所以现在有足够的机会去减少比特精度而不影响准确性。然而，一个十分重要的区别是神经网络对于激进的量化和极端的离散化是非常健壮的。这里新的自由度和包含的参数数量有关，也就是说，我们正在使用过参数化的模型。这对我们是否解决良好条件问题、我们是否对前向误差和反向误差有兴趣等问题有直观影响。在推动量化最新发展的神经网络应用中，没有一个良好条件问题或者良好定态问题正在被解决。相反，有人感兴趣一些前向误差度量的分类，但是对于过参数化，有非常多不同的模型精确或近似地优化了这个度量。因此，一个量化的模型和一个没有量化的模型有着较高的误差是可能的，尽管仍然取得了较好的泛化性能。这个附加的自由度没有体现在很多的经典研究中，它们主要集中于找到不会改变信号太多或者对精确计算和离散计算间的误差有很强控制的压缩方法。这一观察结果一直是研究神经网络量化新技术的主要驱动力。最后，神经网络模型的层级结构提供一个额外的研究维度。神经网络中的不同层对损失函数有着不同的影响，这个鼓励了使用混合精度的方式对神经网络进行量化。

# 量化的基本概念

这个部分，我们首先在3-A节中介绍常用概念和问题建立。然后我们在3-B到3-F节中描述基本的量化概念和方法。其后，我们在3-G节中讨论不同的剪枝方法。紧跟着，3-H节中是随机量化。

## 问题建立和概念

假设神经网络有$L$个有可学习参数的层，记为$\{W_1,W_2,\cdots,W_L\}$，用$\theta$表示所有这些参数的集合。为了不失一般性，我们关注于监督学习问题，名义上的目标是优化下面的经验风险最小化函数：
$$\mathcal{L}(\theta)=1/N\sum_{i=1}^Nl(x_i,y_i;\theta) $$
其中$(x,y)$是输入数据和对应的标签，$l(x,y;\theta)$是损失函数，$N$是数据点的总的个数。让我们也把输入第$i$层的隐藏激活记为$h_i$，相应的输入隐藏激活记为$a_i$。我们假设我们已经训练好模型参数$\theta$，存储为浮点精度数据。在量化过程中，目标是减小参数以及中间激活图的精度，并且最小化对模型的泛化性能和准确度的影响。为了实现这个目标，我们需要去定义一个量化运算，这个量化运算将一个浮点数映射到一个量化的数，我们接下来再讨论这个过程。

## 均匀量化

我们需要首先定义一个能量化神经网络权重和激活到一个有限的数值集合的函数。这个函数接收浮点数，然后把它们映射到低精度范围。一个量化函数的常用选择如下：
$$Q(r)=Int(r/S)-Z$$
这里$Q$是量化操作，$r$是一个实数输入，$S$是一个实数缩放因子，$Z$是整数零点。进一步说，这个函数将实数值映射到一个整数值通过通过取整运算。本质上，这个函数映射实值$r$到一些整数数值。这个量化方式也被称为均匀量化，因为得到的量化结果是均匀的间隔。也有非均匀量化的方法，他们的量化值不一定是均匀间隔的，这些方法将在3-F节中更详细地讨论。从量化值$Q(r)$恢复实数值$r$是可能的，可以通过一个通常被成为反量化的操作：
$$\tilde{r}=S(Q(r)+Z)$$
注意恢复的实数值$\tilde{r}$并不和$r$完全相等，因为有取整运算。

## 对称和非对称量化

均匀量化中一个重要的因素是等式2中的量化因子$S$。这个量化因子实质上将实数值$r$的范围分为了几个部分。
$$S=\frac{\beta-\alpha}{2^b-1}$$
其中$[\alpha,\beta]$表示裁剪范围，是一个我们用来切割实值的有界范围。$b$是量化比特位宽。因此，为了确定量化因子，裁剪范围$[\alpha,\beta]$应该被首先确定。选择裁剪范围的过程经常被称为标定。一个显然的方式是选择信号的最小值或者最大值作为裁剪范围。这个方式是非对称量化，因为裁剪范围不一定关于原点对称。也可以让$-\alpha=\beta$使用对称量化。一个流行的量化是根据输入的最大最小值来选择这些：$-\alpha=\beta =max(|r_{max}|,|r_{min}|)$。非对称量化经常有着相较于对称量化更紧凑的范围。这个是十分重要的，当目标权重或者激活是非均衡的。例如ReLU函数后的数据经常有着非负值。使用对称量化时，可以通过使$Z=0$来简化量化函数为：
$$Q(r)=Int(\frac{r}{S})$$
现在，缩放因子有两个选择。全范围对称量化$S$是$\frac{2max(|r|)}{2^n-1}$（floor rounding），使用整个INT8的范围$[-128,127]$。然而，在“限制范围”$S$是$\frac{max(|r|)}{2^{n-1}-1}$，这只使用了$[-127,127]$的范围。正如预期的，全范围方式更准确。对称量化在权重量化中广泛应用，因为将零点归零可以减少推理过程的计算成本，并且也使得应用更直观。然而，注意到对于激活，由于不对称激活中的中的偏置而占用的交叉项是一个静态的数据独立项。并且可以被吸收在偏置中（或者用于初始化寄存器）。

经常有人使用信号的最大最小值于对称和非对称量化。然而，这种方式易受离群值的影响。可能不必要地增加量化范围，并且降低量化的精度。解决这个的一种方法是使用百分比来替代信号的最大最小值。也就是说，使用第i个最大最小值来替代最大最小值。另一个方式是选择最小化真实值和量化值间的KL分歧的$\alpha$和$\beta$。有兴趣的读者可以看【255】，其中有不同模型上的不同校准方法。

**小结**。对称量化使用对称范围分割了范围。这个的更容易实现，因为使偏置等于0。然而，当范围是倾斜且不对称时，它只是一个备选方案。对于这些情况，不对称量化更好。

## 距离标定算法：静态和动态量化

目前为止，我们讨论了不同的标定算法来划分$[\alpha,\beta]$的切分范围。另一个重要的量化方式的区分是何时确定切分范围。权重的范围可以静态地计算，因为在大多数情况下，参数在推断期间是固定的。然而，每个一个输入对应的激活参数的分布都不同。这样就有两种方式量化激活参数：动态量化和静态量化。

动态量化中，对于每个激活参数分布的量化范围在运行过程中动态地进行计算。这个方式需要对信号的统计数据实时地进行计算，这个会有很高的开销。然而，动态量化经常导致更高的准确率，因为信号范围是对每个输入精确地进行计算的。

另一个量化方式是静态量化，量化范围是预先计算好的，并且在推理过程中保持不变。这个方式不需要添加任何计算开销，但是它典型地更低的精度相比较于动态量化。一个流行的预计算方法是执行一系列的输入标定去计算激活参数的典型范围。为了找到最佳的范围，已经提出了多种不同的度量标准，其中有最小化原始未量化权重分布和对应的量化值之间的均方误差。有些人可能会考虑使用其他的准侧，例如熵，尽管MSE是最为普遍的方法。另一种方式是在神经网络的训练中，学习或者训练量化范围。著名的工作有LQNets【276】，PACT【36】，LSQ【56】和LSQ+【15】，这些在神经网络训练过程中联合地优化量化范围以及权重。

**小结**。动态量化动态地计算每个激活参数的量化范围，并且经常取得最高的准确率。然而动态地计算信号范围的成本十分高昂，从业者们经常使用静态的量化，所有的输入的量化范围是固定的。

## 量化间隔

在大多数计算机视觉任务中，一层的激活输入由许多不同的卷积核卷积。每个这些卷积核有不同的参数范围。这样，这些量化方式的一个区别是如何为权重计算量化范围$[\alpha,\beta]$的粒度。我们如下区分他们。

### 分层量化

在这种方式中，量化范围由一层的所有卷积核权重决定。这里我们检查了这一层中所有参数的统计特性，然后对所有卷积核使用相同的量化范围。尽管这个方式实现起来非常简单，它经常导致非最优的精度，因为每个卷积核的范围可能相差非常大。例如，一个有着更狭窄范围的参数可能会丢失它的量化分辨率，因为相同层中的另一个卷积核有着更宽的范围。

### 分组量化

我们可以对一层中的不同通道进行分组来计算量化范围（对于激活参数或者卷积核）。这可能对每个卷积或者激活参数的分布变化很大的情况很有帮助。例如，这种方式在Q-BERT中对于量化Transformer模型很有帮助，该模型中包含全连接注意力层。然而，这种方式不可避免地带来了考虑不同比例因素的额外开销。

### 分通道量化

一个流行的选择是对于每个通道使用一个独立于其他通道的固定值。也就是说，每个通道分配一个独特的比例因子。这保证了更好的量化分辨率并且经常实现更高的精度。

### 次分通道量化

前面的方法可能比较极端，其中量化范围由一个卷积或者全连接层中的任何参数组有关。然而，这种方式可能添加较多的开销，因为当处理一个卷积层或者全连接层时不同的尺度因子都需要被考虑。因此，分组量化可能建立一个量化分辨率和计算开销之间的平衡。

**小结**。分通道量化是现在量化卷积核的标准方法。它使得研究者们可以用微不足道的开销对每个独特的卷积核进行调整量化范围。相反，次分通道量化可能导致显著地开销，并且不是现在的标准选择。

## 非均匀量化

文献中的一些工作也研究了非均匀量化，量化步长和量化等级都可以是非均匀间隔。非均匀量化的标准化定义在等式6中。其中$X_i$表示离散量化等级并且$\Delta_i$表示量化步长：
$$Q(r)=X_i, if r\in[\Delta_i,\Delta_{i+1})$$
特别的，当真实值$r$落在量化步长$\Delta_i$和$\Delta_{i+1}$之间值，量化函数$Q$将其投射到相应的量化级别$X_i$。注意到$X_i$和$\Delta_i$都不是均匀间隔。

非均匀量化对于一个固定的位宽可能达到更高的精度，因为我们可以通过更关注于重要信息或者找到恰当的动态范围更好地捕获差异。例如，许多非均匀量化方法被设计用于权重和激活数据的钟形分布，这种分布的数据经常包含很长的尾部数据。一个典型的正则化非均匀量化是使用对数分布，其中量化步长和等级以指数形式增长。另一个流行的分支是基于二进制码的量化，其中一个真实值向量$\bold{r}\in\mathbb{R}^n$被量化成m位的向量，通过$r\approx\sum_{i=1}^m\alpha_i\bold{b}_i$，其中$\alpha_i \in \mathbb{R}$并且二元向量$\bold{b}_i\in\{-1,+1\}^n$。因为最小化真值$\bold{r}$和$\sum_{i=1}^m\alpha_i \bold{b}_i$之间的误差的封闭解，之前的研究依赖于启发式解决方案。为了进一步改进量化器，最近的工作将非均匀量化表述为优化问题。作为优化问题的非均匀量化。如式7所示，量化器Q中的量化步骤/电平被调整为最小化原始张量与量化对应量之间的差异。
$$\min_Q||Q(r)-r||^2$$
此外，量化器本身也可以与模型参数联合训练。这些方法被称为可学习量化器，量化步骤/级别通常用迭代优化或梯度下降来训练。

除了基于规则和优化的非均匀量化外，聚类还有助于缓解量化带来的信息损失。一些工作【74,256】使用不同张量上的k-means来确定量化步骤和级别，而另一些工作【38】在权重上应用hessian加权k-means聚类来最小化性能损失。进一步讨论见第4-f节。

**小结**。一般来说，非均匀量化使我们能够更好地捕获信号信息，通过分配位和离散参数的范围非均匀。然而，非均匀量化方案通常难以有效地部署在一般的计算硬件上，如GPU和CPU。因此，由于其简单性和对硬件的有效映射，统一量化是目前事实上的方法。

## 微调的方法

在量化后，经常需要对神经网络中的参数进行调整。这可以通过重新训练模型来完成，这个过程被称为量化感知训练(Quantization-Aware Training, QAT)，也可以不进行重新训练，这个过程通常被称为训练后量化(Post-Training Quantization， PTQ)。这两种方法之间的示意图比较如图4所示，并在下面进一步讨论(有关此主题的更详细讨论，有兴趣的读者请参阅【183】)

### 量化感知训练

给定一个训练过的模型，量化可能会对训练过的模型参数引入扰动，这可能会将模型推离它在浮点精度训练时收敛的点。可以通过用量化参数重新训练神经网络模型来解决这个问题，这样模型就可以收敛到损失更好的点。一种流行的方法是使用Quantization-Aware Training (QAT)，在这种方法中，通常的正向和向后传递在浮点的量化模型上执行，但模型参数在每次梯度更新后被量化(类似于投影梯度下降)。特别重要的是，在以浮点精度执行权重更新之后进行这个投影。使用浮点数执行向后传递很重要，因为在量化精度中积累梯度会导致零梯度或具有高误差的梯度，特别是在低精度中。

反向传播中一个重要的微妙之处是如何处理不可微量化算子(Eq. 2)。在没有任何近似的情况下，该算子的梯度几乎处处为零，因为Eq. 2中的舍入操作是分段平坦算子。解决这个问题的一种流行方法是通过所谓的直通估计器(STE)【13】来近似该算子的梯度。STE实际上忽略了舍入运算，用一个恒等函数逼近它。

尽管STE的近似很粗糙，但在实际应用中，除了二进制量化等超低精度量化外，它通常工作得很好。【271】的工作为这一现象提供了理论证明，并发现STE的粗梯度近似在预期中可以与总体梯度相关(对于STE的适当选择)。从历史的角度来看，我们应该注意到STE的最初想法可以追溯到[209,210]的开创性工作，其中使用了一个单位算子来近似二元神经元的梯度。

虽然STE是主流方法[226,289]，但文献中也探索了其他方法[2,25,31,59,144,164]。我们首先应该提到[13]还提出了一种随机神经元方法作为STE的替代方案。还提出了使用组合优化[65]、目标传播[140]或Gumbel-softmax[116]的其他方法。另一类不同的替代方法尝试使用正则化操作符来强制将权重量化。这样就不需要在Eq. 2中使用不可微量化算子。这些通常被称为非ste方法[4,8,39,99,144,184,283]。该领域最近的研究包括ProxQuant[8]，它删除了量化公式Eq. 2中的四边形操作，而是使用所谓的w形非光滑正则化函数来强制量化值的权重。其他值得注意的研究包括使用脉冲训练来近似不连续点[45]的导数，或者用浮点和量化参数的仿射组合替换量化权重[165]。最近的工作[181]也提出了AdaRound，这是一种自适应舍入方法，可以替代舍入到最接近的方法。尽管这一领域的工作很有趣，但这些方法通常需要大量的调优，到目前为止，STE方法是最常用的方法。

除了调整模型参数外，一些先前的工作发现在QAT中学习量化参数也是有效的。PACT[36]学习均匀量化下激活的剪切范围，而QIT[125]也学习量化步骤和级别，作为对非均匀量化设置的扩展。LSQ[56]引入了一种新的梯度估计来学习QAT期间非负激活(例如ReLU)的比例因子，LSQ+[15]进一步将这一思想扩展到产生负值的一般激活函数，如swish[202]和h-swish [100]。

**小结**。尽管STE有粗近似，但QAT已被证明是有效的。然而，QAT的主要缺点是重新训练神经网络模型的计算成本。这种重新训练可能需要执行几百个epoch来恢复精度，特别是对于低比特精度量化。如果量化模型将在较长一段时间内被部署，并且效率和准确性特别重要，那么在重新训练上的投资可能是值得的。然而，情况并非总是如此，因为有些模型的寿命相对较短。接下来，我们将讨论一种没有这种开销的替代方法。

### 训练后量化

昂贵的QAT方法的另一种替代方法是训练后量化(PTQ)，它执行量化和权重的调整，没有任何微调[11,24,40,60,61,68,69,89,108,142,148,174,182,223,281]。因此，PTQ的开销非常低，通常可以忽略不计。不像QAT需要足够的训练数据进行再训练，PTQ有一个额外的优势，它可以应用在数据有限或未标记的情况下。然而，与QAT相比，这往往是以较低的精度为代价的，特别是对于低精度量化。

因此，人们提出了多种方法来缓解PTQ的精度下降。例如，[11,63]观察权重值量化后的均值和方差的固有偏差，并提出偏差修正方法;并且[174,182]表明，在不同层或通道之间均衡权值范围(以及隐式激活范围)可以减少量化误差。ACIQ[11]分析计算PTQ的最佳剪辑范围和通道位宽设置。虽然ACIQ可以实现较低的精度退化，但在ACIQ中使用的通道激活量化很难有效地部署在硬件上。为了解决这个问题，OMSE方法[40]去除激活时的信道量化，并提出通过优化量化张量与相应浮点张量之间的L2距离来进行PTQ。此外，为了更好地缓解离群值对PTQ的不利影响，[281]提出了一种离群值信道分割(OCS)方法，该方法将包含离群值的信道进行复制和减半。另一项值得注意的工作是AdaRound[181]，该研究表明单纯舍入到最近的量化方法会反直觉地导致次优解，并提出了一种自适应舍入方法，可以更好地减少损失。AdaRound将量化权重的变化限制在与全精度权重的±1范围内，而AdaQuant[108]提出了一种更通用的方法，允许量化权重根据需要变化。PTQ方案可以走向极端，在量化过程中既不使用训练数据也不使用测试数据(又名零镜头场景)，这将在下文中讨论。

**小结**。在PTQ中，所有的权重和激活量化参数都是在没有对神经网络模型进行任何重新训练的情况下确定的。因此，PTQ是一种非常快速的量化神经网络模型的方法。然而，与QAT相比，这往往是以准确性较低为代价的。

### Zero-shot量化

如上所述，为了在量化后实现最小的精度退化，我们需要访问训练数据的一部分的全部。首先，我们需要知道激活的范围，以便我们可以剪辑值并确定适当的缩放因子(在文献中通常称为校准)。其次，量化模型往往需要进行微调，以调整模型参数，恢复精度下降。然而，在许多情况下，在量化过程中无法访问原始训练数据。这是因为训练数据集要么太大而无法分发，要么是专有的(例如，谷歌的JFT-300M)，要么是由于安全或隐私问题而敏感的(例如，医疗数据)。已经提出了几种不同的方法来解决这一挑战，我们称之为零次量化(ZSQ)。受[182]的启发，这里我们首先描述了两种不同级别的零镜头量化:
- 等级1：无数据，无调优(ZSQ + PTQ)
- 等级2：无数据但需要微调(ZSQ + QAT)

级别1允许更快更容易的量化，无需任何微调。调优通常很耗时，而且通常需要额外的超参数搜索。然而，Level 2通常会导致更高的精度，因为微调有助于量化模型恢复精度退化，特别是在超低位精度设置下[85]。[182]的工作使用1级方法，该方法依赖于均衡权重范围和纠正偏差误差，使给定的NN模型更易于量化，而无需任何数据或微调。然而，由于该方法基于(分段)线性激活函数的尺度等方差性质，对于具有非线性激活的神经网络，例如具有GELU[94]激活的BERT[46]或具有swish激活的MobileNetV3[100][203]，它可能是次优的。

ZSQ中一个流行的研究分支是生成与目标预训练模型训练的真实数据相似的合成数据。然后，合成数据用于校准和/或微调量化模型。该领域的早期工作[28]利用生成对抗网络(GANs)[75]进行合成数据生成。使用预训练的模型作为鉴别器，它训练生成器，使其输出可以被鉴别器很好地分类。然后，使用从生成器收集的合成数据样本，可以通过从全精度对应对象中蒸馏知识来对量化模型进行微调(详情请参阅第IV-D节)。然而，这种方法无法捕获真实数据的内部统计数据(例如，中间层激活的分布)，因为它只使用模型的最终输出生成。没有考虑内部统计的合成数据可能不能正确地代表真实的数据分布[85]。为了解决这个问题，许多后续工作使用批处理规范化(BatchNorm)[112]中存储的统计数据，即通道平均值和方差，以生成更真实的合成数据。其中[85]通过直接最小化内部统计量的KL散度来生成数据，并利用合成数据对量化模型进行校准和微调。此外，ZeroQ[24]表明，合成数据可以用于灵敏度测量和校准，从而实现混合精度的训练后量化，而无需访问训练/验证数据。ZeroQ还将ZSQ扩展到对象检测任务，因为它在生成数据时不依赖于输出标签。[85]和[24]都将输入图像设置为可训练参数，直接对其进行反向传播，直到其内部统计量与真实数据相似。进一步来说，最近的研究[37,90,259]发现，训练和利用生成模型能够更好地捕获真实数据分布并生成更真实的合成数据是有效的。

**小结**。Zero Shot(又名无数据)量化在不访问训练/验证数据的情况下执行整个量化。这对于机器学习即服务(MLaaS)提供商来说尤其重要，因为他们希望加快客户工作负载的部署，而不需要访问他们的数据集。此外，这对于安全或隐私问题可能限制对训练数据的访问的情况很重要。

## 随机量化

在推理过程中，量化方案通常是确定的。然而，这并不是唯一的可能性，一些工作已经探索了量化意识训练的随机量化以及降低精度训练[13,79]。高层次的直觉是，与确定性量化相比，随机量化可能允许神经网络探索更多。一个流行的支持论点是，较小的权重更新可能不会导致任何权重变化，因为舍入操作可能总是返回相同的权重。然而，启用随机舍入可以为神经网络提供逃脱的机会，从而更新其参数。

更正式地说，随机量化以与权重更新的幅度相关的概率向上或向下映射浮点数。例如，在[29，79]中，等式2中的Int运算符定义为
$$\text{Int}(x)=\left\{
\begin{array}{cc}
\lfloor x \rfloor &\text{with probability} & \lceil x \rceil-x, \\
\lceil x \rceil & \text{with probability} & x-\lfloor x \rfloor. \\
\end{array} \right. $$
然而，此定义不能用于二进制量化。因此，[42]将此扩展到
$$\text{Binary}(x)=\left\{
\begin{array}{cc}
-1 &\text{with probability} & 1-\sigma(x), \\
+1 & \text{with probability} & \sigma(x). \\
\end{array} \right. $$
其中Binary是将实值$x$二值化的函数，$\sigma(\cdot)$是S形函数。

最近，QuantNoise中引入了另一种随机量化方法[59]。QuantNoise在每个前向传递的随机权重子集，并使用无偏梯度训练模型。这允许在许多计算机视觉和自然语言处理模型中进行较低比特精度的量化，而不会显著降低精度。然而，随机量化方法的一个主要挑战是为每一次权重更新创建随机数的开销，因此它们在实践中尚未被广泛采用。

# 高级概念：8位以下的量化

在本节中，我们将讨论主要用于亚INT8量化的量化中的更高级主题。我们将在第IV-A节中首先讨论模拟量化及其与纯整数量化的差异。随后，我们将在第IV-B节中讨论混合精度量化的不同方法，然后在第IV-C节中讨论硬件感知量化。然后，我们将在第IV-D节中描述如何使用蒸馏来提高量化精度，然后在第IV-E节中讨论极低比特精度量化。最后，我们将在第IV-F节中简要描述矢量量化的不同方法。

## 模拟和仅整数量化

有两种常用的方法来部署量化NN模型，即模拟量化（又名伪量化）和纯整数量化（又名定点量化）。在模拟量化中，量化的模型参数以低精度存储，但运算（例如矩阵乘法和卷积）是用浮点算法执行的。因此，量化参数需要在浮点运算之前进行去量化，如图6（中间）所示。因此，人们不能完全受益于具有模拟量化的快速有效的低精度逻辑。然而，在纯整数量化中，所有操作都使用低精度整数运算[113、132、154、193、267]执行，如图6（右）所示。这允许使用有效的整数运算来执行整个推理，而无需任何参数或激活的浮点去量化。

通常，使用浮点运算以全精度执行推断可能有助于最终量化精度，但这是以无法从低精度逻辑中获益为代价的。与全精度逻辑相比，低精度逻辑在延迟、功耗和区域效率方面具有多重优势。如图7（左侧）所示，许多硬件处理器，包括NVIDIA V100和Titan RTX，支持快速处理低精度算法，从而提高推理吞吐量和延迟。此外，如图7（右）所示，对于45nm技术[97]，低精度逻辑在能量和面积方面明显更有效。例如，与FP32加法相比，执行INT8加法的能量效率高30倍，面积效率高116倍[97]。

值得注意的仅整数量化工作包括[154]，其将批量归一化融合到先前的卷积层中，以及[113]，其提出了一种用于具有批量归一化的残差网络的整数计算方法。然而，这两种方法都限于ReLU激活。[132]的最近工作通过用整数算法逼近GELU[94]、Softmax和层归一化[6]来解决这一限制，并进一步将仅整数量化扩展到Transformer[243]架构。

二进量化是另一类纯整数量化，其中所有缩放都是用二进数进行的，二进数是分子中有整数值，分母中有2次方的有理数[267]。这导致了一个计算图，它只需要整数加法、乘法、移位，而不需要整数除法。重要的是，在这种方法中，所有的加法（例如剩余连接）都被强制为具有相同的二进尺度，这可以使加法逻辑更简单，效率更高。

**小结**。通常，与模拟/伪量化相比，仅整数和二进量化更可取。这是因为整数运算只使用较低精度的逻辑，而模拟量化使用浮点逻辑来执行运算。然而，这并不意味着假量化永远不会有用。事实上，伪量化方法对于带宽受限而非计算受限的问题是有益的，例如在推荐系统中[185]。对于这些任务，瓶颈是内存占用和从内存加载参数的成本。因此，对于这些情况，执行伪量化是可以接受的。

## 混合精度量化

很容易看出，当我们使用较低精度的量化时，硬件性能有所提高。然而，将模型统一量化到超低精度会导致显著的精度下降。可以通过混合精度量化[51、82、102、162、187、199、211、239、246、249、263、282、286]来解决这一问题。在这种方法中，每个层都以不同的比特精度进行量化，如图8所示。这种方法的一个挑战是，用于选择此位设置的搜索空间在层数上是指数级的。已经提出了不同的方法来解决这个巨大的搜索空间。

为每一层选择这种混合精度本质上是一个搜索问题，已经提出了许多不同的方法。[246]的最近工作提出了一种基于增强学习（RL）的方法来自动确定量化策略，并且作者使用硬件模拟器在RL代理反馈中获取硬件加速器的反馈。论文[254]将混合精度配置搜索问题表述为神经架构搜索（NAS）问题，并使用可区分NAS（DNAS）方法有效地探索搜索空间。这些基于探索的方法[246、254]的一个缺点是它们通常需要大量的计算资源，并且它们的性能通常对超参数甚至初始化敏感。

另一类混合精度方法使用周期函数正则化来训练混合精度模型，方法是在学习其各自的比特宽度的同时，自动区分不同的层及其在精度方面的不同重要性[184]。

与这些基于探索和规则化的方法不同，HAWQ[51]引入了一种基于模型二阶灵敏度的混合精度设置的自动方法。理论上表明，二阶算子（即黑森算子）的轨迹可用于测量层对量化的敏感性[50]，类似于最佳脑损伤的开创性工作中的修剪结果[139]。在HAWQv2中，该方法被扩展到混合精度激活量化[50]，并被证明比基于RL的混合精度方法快100倍以上[246]。最近，在HAWQv3中，引入了仅整数的硬件感知量化[267]，该量化提出了一种快速整数线性规划方法，以找到给定应用特定约束（例如，模型大小或延迟）的最佳比特精度。这项工作还通过将混合精度量化直接部署在T4 GPU上，解决了混合精度量化的硬件效率的常见问题，与INT8量化相比，混合精度（INT4/INT8）量化的速度提高了50%。

**小结**。对于不同神经网络模型的低精度量化，混合精度量化被证明是一种有效且硬件高效的方法。在这种方法中，NN的层被分组为对量化敏感/不敏感的层，并且每个层使用较高/较低的比特。因此，可以将精度降低降到最低，并仍然受益于减少的内存占用和低精度量化的更快速度。最近的工作[267]还表明，由于混合精度仅在操作/层之间使用，因此该方法具有硬件效率。

## 硬件感知量化

量化的目标之一是提高推理延迟。然而，在量化某一层/操作后，并非所有硬件都提供相同的速度。事实上，量化的好处取决于硬件，许多因素如片上内存、带宽和缓存层次结构都会影响量化速度。

考虑这一事实对于通过硬件感知量化实现最佳效益非常重要[87，91，246，250，254，256，265，267]。特别地，工作[246]使用增强学习代理来基于关于具有不同比特宽度的不同层的延迟的查找表来确定用于量化的硬件感知混合精度设置。然而，这种方法使用模拟的硬件延迟。为了解决这一问题，[267]的最新工作直接在硬件中部署量化操作，并针对不同量化比特精度测量每个层的实际部署延迟。

## 蒸馏辅助量化

量化中一项有趣的工作是结合模型蒸馏来提高量化精度[126，177，195，267]。模型蒸馏[3，95，150，177，195，207，268，270，289]是一种方法，其中使用具有更高精度的大模型作为教师，以帮助训练紧凑的学生模型。在学生模型的训练过程中，模型蒸馏建议利用教师产生的软概率，而不是仅使用基本事实类标签，这可能包含更多的输入信息。即总损失函数包含学生损失和蒸馏损失，通常公式如下：
$$\mathcal{L}=\alpha\mathcal{H}(y,\sigma(z_s))+\beta\mathcal{H}(\sigma(z_t,T),\sigma(z_s,T))$$
在方程10中，$\alpha$和$\beta$是调整学生模型损失量和蒸馏损失的加权系数，$y$是真值类标签，$\mathcal{H}$是交叉熵损失函数，$z_s/z_t$是学生/教师模型生成的对数，$\sigma$是softmax函数，$T$是其温度，定义如下：
$$p_i=\frac{\text{exp}\frac{z_i}{T}}{\sum_j\text{exp}\frac{z_j}{T}}$$

以往的知识蒸馏方法侧重于探索不同的知识来源。[95，150，192]使用逻辑（软概率）作为知识的来源，而[3，207，269]试图利用来自中间层的知识。教师模型的选择也得到了很好的研究，[235，273]使用多个教师模型来共同监督学生模型，而[43，277]在没有额外教师模型的情况下应用自我蒸馏。

## 极端量化

二进制化（Binarization）是最极端的量化方法，其中量化值被限制为1位表示，从而大幅减少32×的内存需求。除了存储器的优点之外，二进制（1位）和三进制（2位）运算通常可以用逐位算法有效地计算，并且可以在更高精度（例如FP32和INT8）上实现显著的加速。例如，NVIDIA V100 GPU上的峰值二进制运算比INT8高8倍。然而，简单的二值化方法将导致显著的精度下降。因此，有大量工作提出了不同的解决方案[18、25、47、52、77、78、83、92、93、120、122、124、129、131、135、141、149、155、160、196、198、205、217、249、251、260、262、288、290]。

这里的一项重要工作是BinaryConnect[42]，它将权重限制为+1或-1。在这种方法中，权重保持为真实值，并且仅在向前和向后过程中进行二值化，以模拟二值化效果。在正向传递过程中，实值权重根据符号函数转换为+1或-1。然后，可以使用标准的STE训练方法来训练网络，以通过不可微符号函数传播梯度。二值化NN[107]（BNN）通过二值化激活和权重来扩展这一思想。联合二值化权重和激活具有提高延迟的额外好处，因为代价高昂的浮点矩阵乘法可以用轻量级的XNOR操作代替，然后进行位计数。另一项有趣的工作是[45]中提出的二进制权重网络（BWN）和XNORNet，它们通过将缩放因子纳入权重并使用+α或-α代替+1或-1来实现更高的精度。这里，$\alpha$是选择的缩放因子，以最小化实值权重和生成的二值化权重之间的距离。换句话说，实值权重矩阵$W$可以公式化为$W\approx\alpha B$，其中$B$是满足以下优化问题的二进制权重矩阵：
$$\alpha,B=\text{argmin}||W-\alpha B||^2$$

此外，受许多学习到的权重接近于零的观察结果的启发，有人试图通过用三元值（例如+1、0和-1）约束权重/激活来对网络进行内部化，从而明确允许量化值为零[145、159]。三元化还通过消除像二元化那样昂贵的矩阵乘法，大大减少了推理延迟。后来，三元二元网络（TBN）[244]表明，将二元网络权重和三元激活相结合可以实现精度和计算效率之间的最佳权衡。

由于原始二值化和三值化方法通常会导致严重的精度下降，特别是对于像ImageNet分类这样的复杂任务，已经提出了许多解决方案来减少极端量化中的精度下降。[197]的工作将这些解决方案大致分为三个分支。在这里，我们简要讨论了每个分支，感兴趣的读者可以参考[197]了解更多细节。

### 量化误差最小化

解决方案的第一分支旨在最小化量化误差，即实数值和量化值之间的间隙[19，34，62，103，151，158，164，169，178，218，248]。HORQ[151]和ABC-Net[158]使用多个二进制矩阵的线性组合，即W≈α1B1+···+αMBM，来减少量化误差，而不是使用单个二进制矩阵来表示实值权重/激活。受激活二值化降低了其对后续卷积块的表示能力这一事实的启发，[178]和[34]表明，更宽网络（即，具有更大数量滤波器的网络）的二值化可以实现精度和模型大小之间的良好权衡。

### 改进的损失函数

另一个工作分支侧重于损失函数的选择[48，98，99，251，284]。这里的重要工作是损失感知二值化和三值化[98，99]，它们直接最小化了二值化/三值化权重的损失。这与其他仅近似权重而不考虑最终损失的方法不同。从全精度教师模型中提取知识也被证明是恢复二值化/三值化后精度下降的一种有前途的方法。

### 改进的训练方法

另一个有趣的工作分支旨在改进二元/三元模型的训练方法[5，20，44，73，160，164，285，288]。许多努力指出了STE在通过符号函数反向传播梯度中的局限性：STE仅传播[-1，1]范围内的权重和/或激活的梯度。为了解决这一问题，BNN+[44]引入了符号函数导数的连续逼近，而[198，261，272]用平滑的可微函数代替符号函数，该函数逐渐变尖并接近符号函数。Bi Real Net[164]引入了将激活连接到连续块中的激活的身份快捷方式，通过该快捷方式可以传播32位激活。虽然大多数研究集中于减少推断时间延迟，但DoReFa-Net[285]除了权重和激活外，还量化了梯度，以加速训练。

极端量化已经成功地大幅减少了计算机视觉任务中许多CNN模型的推理/训练延迟以及模型大小。最近，有人试图将这一想法扩展到自然语言处理（NLP）任务[7，119，121，278]。考虑到在大量未标记数据上预训练的最先进NLP模型（例如BERT[46]、RoBERTa[163]和GPT家族[17200201]）的令人望而却步的模型大小和推理延迟，极限量化正在成为将NLP推理任务推向边缘的有力工具。

**小结**。极低比特精度量化是一个非常有前途的研究方向。然而，与基线相比，现有方法通常会导致高精度退化，除非执行非常广泛的调谐和超参数搜索。但对于不太关键的应用，这种精度下降可能是可以接受的。

## 向量量化

正如第二节所讨论的，量化并没有在机器学习中被发明，但在过去的一个世纪中，量化在信息理论中得到了广泛的研究，尤其是在数字信号处理领域中作为一种压缩工具。然而，用于机器学习的量化方法之间的主要区别在于，与原始信号相比，我们根本不感兴趣以最小变化/误差压缩信号。相反，我们的目标是找到一种降低精度的表示，从而尽可能减少损失。因此，如果量化权重/激活远离非量化权重，则完全可以接受。

话虽如此，DSP中的经典量化方法中有许多有趣的想法，这些方法已应用于NN量化，尤其是矢量量化[9]。特别地，[1，30，74，84，117，170，180，189，256]的工作将权重聚类为不同的组，并在推断期间使用每个组的质心作为量化值。如等式13所示，$i$是张量中的权重下标，$c_1,\dots,c_k$是通过聚类找到的$k$个质心，$c_j$是$w_i$的对应质心。在聚类之后，权重$w_i$将具有与码本（查找表）中的$c_j$相关的聚类索引$j$。
$$\min_{c_1,\dots,c_k}\sum_i||w_i-c_j||^2$$
已经发现，使用k均值聚类足以将模型大小减少到8×，而不会显著降低精度[74]。除此之外，联合应用基于k均值的向量量化以及修剪和霍夫曼编码可以进一步减小模型大小[84]。

乘积量化[74，227，256]是矢量量化的扩展，其中权重矩阵被划分为子矩阵，并且矢量量化被应用于每个子矩阵。除了基本的乘积量化方法之外，更细粒度地使用聚类可以进一步提高精度。例如，在[74]中，进一步递归量化k均值乘积量化后的残差。在[189]中，作者将更多的簇应用于更重要的量化范围，以更好地保存信息。

# 量化和硬件处理

我们已经说过，量化不仅减少了模型的大小，而且还实现了更快的速度和更少的功耗，特别是对于具有低精度逻辑的硬件。因此，量化对于物联网和移动应用的边缘部署尤为重要。边缘设备通常有严格的资源限制，包括计算、内存，重要的是电源预算。对于许多深度神经网络模型来说，这些成本往往太高。此外，许多边缘处理器不支持浮点运算，特别是在微控制器中。

这里，我们简要讨论了量化背景下的不同硬件平台。ARM Cortex-M是一组32位RISC ARM处理器内核，专为低成本和节能的嵌入式设备而设计。例如，STM32系列是基于ARM Cortex-M内核的微控制器，也用于边缘的神经网络推理。由于一些ARM Cortex-M内核不包括专用的浮点单元，因此在部署之前应首先对模型进行量化。CMSIS-NN[136]是ARM的一个库，帮助量化NN模型并将其部署到ARM Cortex-M内核上。具体而言，该库利用具有两个缩放因子的幂的定点量化[113、154、267]，使得可以通过比特移位操作有效地执行量化和去量化过程。GAP-8[64]是一种RISC-V SoC（片上系统），用于使用专用CNN加速器进行边缘推断，是仅支持整数运算的边缘处理器的另一个示例。虽然可编程通用处理器因其灵活性而被广泛采用，但谷歌边缘TPU，一种专门构建的ASIC芯片，是另一种在边缘运行推理的新兴解决方案。与在拥有大量计算资源的谷歌数据中心运行的云TPU不同，Edge TPU是为小型低功耗设备设计的，因此它只支持8位运算。NN模型必须使用TensorFlow的量化感知训练或训练后量化进行量化。

图9绘制了不同商业边缘处理器的吞吐量，这些处理器被广泛用于边缘的神经网络推理。在过去的几年里，边缘处理器的计算能力有了显著的提高，这允许部署和推断以前只能在服务器上使用的昂贵的神经网络模型。量化与高效的低精度逻辑和专用的深度学习加速器相结合，是这种边缘处理器发展的一个重要推动力。

虽然量化是许多边缘处理器不可或缺的技术，但它也可以为非边缘处理器带来显著的改进，例如，满足服务水平协议（SLA）要求，如第99百分位延迟。最近的NVIDIA图灵GPU提供了一个很好的例子，特别是T4 GPU，其中包括图灵张量核心。Tensor核心是专门为高效低精度矩阵乘法而设计的执行单元。

# 量化研究的未来方向

在这里，我们简要讨论了未来量化研究的几个高水平挑战和机遇。这分为量化软件、硬件和神经网络架构的共同设计、耦合压缩方法和量化训练。

**量化软件**：使用当前的方法，在不损失准确性的情况下，可以直接量化不同的神经网络模型并将其部署到INT8。有几个软件包可用于部署INT8量化模型（例如，Nvidia的TensorRT、TVM等），每个软件包都有良好的文档。此外，实现方式也是非常优化的，并且可以容易地观察到量化的加速。然而，用于较低比特精度量化的软件并不是广泛可用的，并且有时它是不存在的。例如，英伟达的TensorRT目前不支持亚INT8量化。此外，最近才将对INT4量化的支持添加到TVM[267]中。最近的工作表明，INT4/INT8的低精度和混合精度量化在实践中有效[51，821021081871991211239246246249263267286]。因此，开发用于低精度量化的高效软件API将产生重要影响。

**硬件与神经网络架构的协同设计**：如上所述，低精度量化中的经典工作与机器学习中的最新工作之间的一个重要区别是，NN参数可能具有非常不同的量化值，但仍然可以很好地推广。例如，通过量化感知训练，我们可能会收敛到不同的解，远离具有单个精度参数的原始解，但仍然获得良好的精度。人们可以利用这种自由度，并在对NN架构进行量化时对其进行调整。例如，[34]的最新工作表明，改变神经网络架构的宽度可以减少/消除量化后的泛化差距。未来的一项工作是在模型被量化时，联合调整其他架构参数，如深度或单个内核。
未来的另一项工作是将这种协同设计扩展到硬件体系结构。这可能对FPGA部署特别有用，因为可以探索许多不同的可能硬件配置（例如乘累加元件的不同微体系结构），然后将其与NN体系结构和量化协同设计相结合。

**耦合压缩方法**：如上所述，量化只是有效部署神经网络的方法之一。其他方法包括高效的神经网络架构设计、硬件和神经网络架构的联合设计、修剪和知识提取。量化可以与这些其他方法相结合。然而，目前探索这些方法的最佳组合的工作很少。例如，修剪和量化可以一起应用于模型，以减少其开销[87152]，理解结构化/非结构化修剪和量化的最佳组合很重要。类似地，另一个未来方向是研究这些方法与上述其他方法之间的耦合。

**量化训练**：也许量化最重要的用途是以半精度加速NN训练[41，72，79175]。这使得能够使用更快、更节能的低精度逻辑进行训练。然而，要将这一点进一步归结为INT8精确训练是非常困难的。虽然在这一领域存在一些有趣的工作[10，26123137173]，但所提出的方法通常需要大量的超参数调整，或者它们只适用于相对简单的学习任务中的少数神经网络模型。基本问题是，在INT8精度的情况下，训练可能会变得不稳定和发散。应对这一挑战可能会对多个应用程序产生重大影响，尤其是在边缘培训方面。

# 总结和结论

一旦抽象数学计算适应了数字计算机上的计算，就出现了这些计算中数值的有效表示、操作和通信问题。与数字表示问题密切相关的是量化问题：一组连续的实数应该以什么方式分布在一组固定的离散数字上，以最大限度地减少所需的位数，并最大限度地提高伴随计算的准确性？虽然这些问题与计算机科学一样古老，但这些问题与高效神经网络模型的设计尤其相关。这有几个原因。首先，神经网络是计算密集型的。因此，数值的有效表示尤为重要。其次，目前大多数神经网络模型都严重过度参数化。因此，有充分的机会在不影响精度的情况下降低钻头精度。第三，神经网络模型的分层结构提供了一个需要探索的额外维度。因此，神经网络中的不同层对损失函数有不同的影响，这激发了诸如混合精度量化之类的有趣方法。

从浮点表示转移到以八位或四位以下表示的低精度固定整数值有可能减少内存占用和延迟。[157]表明，使用TVM[32]量化库对包括ResNet50[88]、VGG-19[224]和inceptionV3[230]在内的流行计算机视觉模型进行INT8推理，可以在NVIDIA GTX 1080上分别实现3.89倍、3.32倍和5.02倍的加速。[213]进一步表明，与INT8推理相比，ResNet50的INT4推理可以在NVIDIA T4和RTX上带来50-60%的额外加速，强调了使用较低比特精度以最大限度地提高效率的重要性。最近，[267]利用混合精度量化，与没有精度下降的INT8推理相比，ResNet50实现了23%的加速，[132]将仅INT8推理扩展到BERT模型，使推理速度比FP32快4.0倍。虽然上述工作侧重于GPU上的加速，但[114]还通过各种计算机视觉模型的INT8量化，在Intel Cascade Lake CPU和Raspberry Pi4（均为非GPU架构）上分别获得了2.35倍和1.40倍的延迟加速。因此，正如我们的参考文献所证明的那样，神经网络模型中的量化问题一直是一个非常活跃的研究领域。

在这项工作中，我们试图为这些非常多样化的努力带来一些概念结构。我们首先讨论了许多量化应用中常见的主题，如均匀、非均匀、对称、非对称、静态和动态量化。然后，我们考虑了对于神经网络的量化来说更独特的量化问题。其中包括分层量化、分组量化、信道量化和子信道量化。我们进一步考虑了训练和量化之间的相互关系，并讨论了量化意识训练与训练后量化相比的优缺点。进一步讨论量化和训练之间的关系是数据的可用性问题。这种情况的极端情况是，由于隐私等各种合理的原因，训练中使用的数据不再可用。这引发了零样本量化的问题。

由于我们特别关注针对边缘部署的高效NN，我们考虑了这种环境特有的问题。其中包括量化技术，其导致由少于8个比特表示的参数，可能低至二进制值。我们还考虑了纯整数量化的问题，这使得能够在通常缺乏浮点单元的低端微处理器上部署NN。

通过这项调查及其组织，我们希望对神经网络量化方面的当前研究提供一个有用的快照，并提供一个智能组织，以方便对该领域未来研究的评估。